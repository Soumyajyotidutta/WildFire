# -*- coding: utf-8 -*-
"""WildFire - Submission

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lxrf8FtDf2T4x3w1j7c6kxueS1fkSRqT

#Install Dependencies and Import Libraries
"""

import sys
!{sys.executable} -m pip install geopandas

import sys
!{sys.executable} -m pip install geopy

import pandas as pd
import numpy as np
from datetime import timedelta
import geopandas as gpd
import matplotlib.pyplot as plt
from shapely.geometry import Point, Polygon, MultiPoint
from geopy.distance import distance
from pyproj import Proj, transform
import requests
import json

"""# Load Dataset"""

from google.colab import drive
drive.mount('/content/drive')

BASE_PATH = '/content/drive/MyDrive/WILDFIRE/Database/Wildfire_updt'

## read in data that has more information on each fire
fire= pd.read_csv(BASE_PATH + '/Wildland_Fire_Incident_Locations.csv')

"""# Format Data to Desired Forms"""

## this is the NASA satalite data
firesat = pd.read_csv(BASE_PATH + "/fire.csv")

## making coordinates out of lat and long points
fire['coords']=list(zip(fire['X'],fire['Y']))
fire['coords'] = fire['coords'].apply(Point)
fire['initial_coords']=list(zip(fire['InitialLongitude'],fire['InitialLatitude']))
fire['initial_coords']=fire['initial_coords'].apply(Point)

firesat['coords']=list(zip(firesat['longitude'],firesat['latitude']))
firesat['coords'] = firesat['coords'].apply(Point)

for i in ['FireDiscoveryDateTime', 'ICS209ReportDateTime', 'ICS209ReportForTimePeriodFrom', 'ICS209ReportForTimePeriodTo']:
    fire[i]=pd.to_datetime(fire[i])

f = fire[['IncidentName', 'FireDiscoveryDateTime', 'ICS209ReportDateTime', 'ICS209ReportForTimePeriodFrom', 'ICS209ReportForTimePeriodTo']]
f=f.groupby(['IncidentName'])['FireDiscoveryDateTime'].min()
f=pd.DataFrame(f)
f.reset_index(inplace=True)
fire = fire.merge(f, how='left', on='IncidentName')
fire = fire[fire['FireDiscoveryDateTime_y']>='2019-01-01']
fire = fire[fire['FireDiscoveryDateTime_y']<'2020-01-01']
for i in ['ContainmentDateTime', 'FireOutDateTime', 'InitialResponseDateTime']:
    fire[i]=pd.to_datetime(fire[i])
f= fire.groupby(['IncidentName'])['ContainmentDateTime'].min()
f=pd.DataFrame(f)
f.reset_index(inplace=True)
fire = fire.merge(f, how='left', on='IncidentName')

f= fire.groupby(['IncidentName'])['FireOutDateTime'].min()
f=pd.DataFrame(f)
f.reset_index(inplace=True)
fire = fire.drop('FireOutDateTime', axis=1)
fire = fire.merge(f, how='left', on='IncidentName')

f= fire.groupby(['IncidentName'])['InitialResponseDateTime'].mean()
f=pd.DataFrame(f)
f.reset_index(inplace=True)
fire = fire.drop('InitialResponseDateTime', axis=1)
fire = fire.merge(f, how='left', on='IncidentName')

f= fire.groupby(['IncidentName'])['DiscoveryAcres'].min()
f=pd.DataFrame(f)
f.reset_index(inplace=True)
fire = fire.drop('DiscoveryAcres', axis=1)
fire = fire.merge(f, how='left', on='IncidentName')
fire = fire.drop(columns=['ContainmentDateTime_x', 'FireDiscoveryDateTime_x', 'ICS209ReportDateTime', 'ICS209ReportForTimePeriodFrom', 'ICS209ReportForTimePeriodTo','ControlDateTime'])

fire1 = fire[['IncidentName', 'DiscoveryAcres', 'FireBehaviorGeneral', 'FireBehaviorGeneral1', 'FireCause', 'FireCauseGeneral', 'FireCauseSpecific', 'FireOutDateTime', 'InitialResponseAcres', 'InitialResponseDateTime', 'PredominantFuelGroup', 'PrimaryFuelModel', 'SecondaryFuelModel', 'FireDiscoveryDateTime_y', 'ContainmentDateTime_y', 'coords']]

fire1.head()

fire1 = fire1.dropna(subset=['IncidentName', 'PrimaryFuelModel'])

groups = fire1.groupby('IncidentName')
for IncidentName, group in groups:
    for col in group.columns[group.isnull().any()]:
        non_missing = group.loc[group[col].notnull(), col]
        if len(non_missing)>0:
            fire1.loc[group.index, col] = non_missing.iloc[0]

fire1=fire1.dropna(subset=['ContainmentDateTime_y', 'FireOutDateTime'], how='all')
enddate = {'ContainmentDateTime_y':fire1['FireOutDateTime'], 'FireOutDateTime':fire1['ContainmentDateTime_y']}
fire1 = fire1.fillna(value=enddate)

fire2=fire1[['IncidentName', 'DiscoveryAcres', 'FireCause', 'FireOutDateTime', 'PrimaryFuelModel', 'SecondaryFuelModel', 'FireDiscoveryDateTime_y', 'ContainmentDateTime_y', 'coords']]
fire2 = fire2.sort_values(['IncidentName', 'DiscoveryAcres'], ascending=False)

fire3 = fire2.drop_duplicates(subset=['IncidentName', 'FireCause', 'PrimaryFuelModel', 'SecondaryFuelModel'], keep='first')

fire3['SecondaryFuelModel'] = fire3['SecondaryFuelModel'].fillna(fire3['PrimaryFuelModel'])

fire3=fire3.drop_duplicates(subset='IncidentName', keep=False)

## read the fire perimeter data into a GeoDataFrame
fireper = gpd.GeoDataFrame.from_file(BASE_PATH + "/EventPolygon2019.shp")

fireper = fireper[fireper['MapMethod']!="Hand Sketch"]

sum(fireper.geometry.isna())

fireper.dropna(subset=['geometry'], inplace=True)

fireper_area_sorted = fireper.sort_values('SHAPE_Area', kind = 'mergesort', ascending = False)
fireper_area_sorted['area_sqKM'] = fireper_area_sorted['geometry'].to_crs({'init': 'epsg:3395'})\
               .map(lambda p: p.area / 10**6)
fireper_area_sorted.head() # SHAPE_Area = cylindrical area, area_sqKM is normal area

fireper_area_sorted.head(1)

fireper_sorted_areasqkm = fireper_area_sorted.sort_values('area_sqKM', ascending = False)
fireper_sorted_areasqkm.info()

fireper_sorted_areasqkm['IncidentNa'].dropna(axis = 0).unique()[0:12]

fireper_sorted_areasqkm['IncidentNa'].dropna(axis = 0).unique()[-12]

fireper_sorted_areasqkm.dropna(axis = 0).iloc[-1,:]

# fireper_area_sorted1 =fireper_area_sorted.copy()
# fireper_area_sorted1 = fireper_area_sorted.to_crs({'init': 'epsg:3857'})
# print(fireper_area_sorted1.crs)
# fireper_area_sorted1.head()

# fireper_area_sorted1['area_sqKM'] = fireper_area_sorted1['geometry'].area/10**6
# fireper_area_sorted1.head()

print(-.05*fireper['SHAPE_Area'].std()+fireper['SHAPE_Area'].mean())

print(sum(fireper['SHAPE_Area'].isna()))
print(sum(fireper['IncidentNa'].isna()))

print(sum(fireper['SHAPE_Area']<0.0000000245))
#fireper1 = fireper[fireper['SHAPE_Area']>0.0000000245]

print(fireper['IncidentNa'].duplicated().sum())

## If the polygon has a hole in it, then this fills the hole
filled_fireper = gpd.GeoDataFrame(columns=fireper.columns)
for index, row in fireper.iterrows():
    if row['geometry'].type == 'Polygon' and len(row['geometry'].interiors)>0:
        exterior_coords = list(row['geometry'].exterior.coords)
        interior_coords = []
        for hole in row['geometry'].interiors:
            interior_coords.extend(list(hole.coords))
        new_polygon = Polygon(exterior_coords + interior_coords)
        filled_fireper = filled_fireper.append({'geometry': new_polygon}, ignore_index=True)
    else:
        filled_fireper = filled_fireper.append(row, ignore_index=True)

filled_fireper1=filled_fireper[filled_fireper['DeleteThis']=='No']
filled_fireper1=filled_fireper1.drop(columns="DeleteThis")

filled_fireper1.to_file("filled_fireper1.shp")

filled_fireper = gpd.GeoDataFrame.from_file("filled_fireper1.shp")

filled_fireper.dropna(subset=['geometry', 'SHAPE_Area', 'GISAcres'], inplace=True)
filled_fireper = filled_fireper[filled_fireper['GISAcres']>0]
filled_fireper = filled_fireper[filled_fireper['SHAPE_Area']>0]

filled_fireper['missing_data'] = filled_fireper.isnull().sum(axis=1)

filled_fireper = filled_fireper.sort_values(['IncidentNa', 'SHAPE_Area', 'missing_data'], ascending=False)

for i in ['CreateDate', 'GDB_TO_DAT', 'GDB_FROM_D', 'DateCurren']:
    filled_fireper[i]=pd.to_datetime(filled_fireper[i])

filled_fireper.iloc[110]

filled_fireper.info()

ff = filled_fireper[['IncidentNa', 'CreateDate', 'GDB_TO_DAT', 'GDB_FROM_D', 'SHAPE_Area']].drop_duplicates(subset=None, keep='first')
ff = ff.sort_values('CreateDate').groupby(by='IncidentNa')

ff.head()

unique_incident_names = ff['IncidentNa'].unique()

incident_to_min_date = {}

filled_fireper.info()

for incident in unique_incident_names:
  all_data_for_incident = filled_fireper[filled_fireper['IncidentNa'] == incident[0]]

  all_dates = []
  all_dates.append(min(all_data_for_incident['CreateDate']))
  all_dates.append(min(all_data_for_incident['DateCurren']))
  all_dates.append(min(all_data_for_incident['GDB_FROM_D']))
  all_dates.append(min(all_data_for_incident['GDB_TO_DAT']))

  min_date_for_incident = min(all_dates)

  incident_to_min_date[incident[0]] = min_date_for_incident

filled_fireper['minDate'] = None

for incident in unique_incident_names:
  temp = filled_fireper[filled_fireper['IncidentNa'] == incident[0]]
  temp['minDate'] = incident_to_min_date[incident[0]] 
  filled_fireper[filled_fireper['IncidentNa'] == incident[0]] = temp

filled_fireper['minDate']

filled_fireper['merged_poly'] = None

from shapely.ops import unary_union

for incident in unique_incident_names:
  temp = filled_fireper[filled_fireper['IncidentNa'] == incident[0]]
  merged_poly = unary_union(temp.geometry)

  temp['merged_poly'] = merged_poly
  filled_fireper[filled_fireper['IncidentNa'] == incident[0]] = temp

filled_fireper.info()

date_data = filled_fireper[['OBJECTID', 'IncidentNa', 'CreateDate', 'DateCurren', 'GDB_FROM_D', 'GDB_TO_DAT', 'minDate']]

date_data.to_csv(BASE_PATH + '/filled_fireper_DATE_data_MARCH_30.csv')

filled_fireper = filled_fireper.drop(['CreateDate', 'DateCurren', 'GDB_FROM_D', 'GDB_TO_DAT', 'minDate'], axis=1)

filled_fireper[['OBJECTID', 'IncidentNa', 'geometry']].to_file(BASE_PATH + '/filled_fireper_NOT_MERGED_MARCH_30.shp')

temp = filled_fireper

temp['geometry'] = temp['merged_poly']

filled_fireper[['OBJECTID', 'IncidentNa', 'geometry']].to_file(BASE_PATH + '/filled_fireper_MERGED_POLY_MARCH_30.shp')

temp2 = filled_fireper.drop(['geometry', 'merged_poly'], axis=1)

temp2.to_csv(BASE_PATH + '/filled_fireper_NON_POLY_DATADATE_data_MARCH_30.csv')

filled_fireper.info()

ff = filled_fireper.groupby(by='IncidentNa', axis=0).max('SHAPE_Area')
ff.head()

fire.head()

fire['FireDiscoveryDateTime']=pd.to_datetime(fire['FireDiscoveryDateTime'])
fire['ContainmentDateTime']=pd.to_datetime(fire['ContainmentDateTime'])

fire2 = fire.groupby(by='IncidentName', axis=0).min('FireDiscoveryDateTime')
fire2.head()

filled_fireper['expanded']=filled_fireper['geometry'].buffer(.00009)

len(filled_fireper1.columns)

filled_fireper1['overlap'] = 0
for i in range(len(filled_fireper1)):
    for j in range(i+1, len(filled_fireper1)):
        if (filled_fireper1.iloc[i, 2]!= filled_fireper1.iloc[j, 2]) and (filled_fireper1.iloc[i, 30]==0) and (filled_fireper1.iloc[j, 30]==0):
            if filled_fireper1.iloc[i, 28].intersects(filled_fireper1.iloc[j, 28]):
                filled_fireper1.iloc[i, 30]=j

id = filled_fireper[['OBJECTID', 'IncidentNa', 'GISAcres', 'GlobalID', 'SHAPE_Area', 'IRWINID']]
id.to_csv('fireperid.csv')

id = filled_fireper[['OBJECTID', 'IncidentNa', 'GISAcres', 'GlobalID', 'SHAPE_Area', 'IRWINID', 'overlap']]
id.to_csv('fireperid.csv')

"""## Merging the Fire Sat Data set (Nasa) with the Polygon Dataset"""

fireper = gpd.GeoDataFrame.from_file(BASE_PATH+"/filled_fireper_MERGED_POLY_MARCH_30.shp")

fireper = fireper.astype({'OBJECTID':int})
fireper.tail()

fireper_nonan = fireper.dropna(axis=0)
fireper_nonan.head()

fireper1 = pd.read_csv(BASE_PATH + '/filled_fireper_NON_POLY_DATA_MARCH_30.csv')

fireper1 = fireper1[['IncidentNa', 'OBJECTID', 'GISAcres', 'IsVisible', 'IRWINID', 'SHAPE_Area']]

fireper1['missing_data'] = fireper1.isnull().sum(axis=1)

fireper1 = fireper1.sort_values(['IncidentNa', 'SHAPE_Area', 'missing_data'], ascending=False)
fireper2=fireper1.drop_duplicates(subset='IncidentNa', keep='first')

fireper2= pd.merge(fireper2, fire3, how='inner', left_on='IncidentNa', right_on='IncidentName')

firesat['acq_date']=pd.to_datetime(firesat['acq_date'])

firesat = firesat[(firesat['acq_date']>='2019-01-01') & (firesat['acq_date']<'2020-01-01')]

points = gpd.GeoDataFrame(firesat, geometry='coords', crs=fireper.crs)

wf = pd.merge(fireper, fireper2, on='OBJECTID', how='inner')

wf['FireDiscoveryDateTime_y']=wf['FireDiscoveryDateTime_y']-pd.Timedelta(days=1)
wf['FireOutDateTime']=wf['FireOutDateTime']+pd.Timedelta(days=2)

wf['FireDiscoveryDateTime_y'] = wf['FireDiscoveryDateTime_y'].apply(lambda x:x.tz_localize(None))

wf1 = wf.iloc[[0]]
pointsinfire = gpd.tools.sjoin(points, wf1, predicate="within", how='inner')
pointsinfire = pointsinfire[(pointsinfire['acq_date']>=pointsinfire['FireDiscoveryDateTime_y']) & (pointsinfire['acq_date']<=pointsinfire['FireOutDateTime'])]

wf = wf.iloc[1:]
for i in range(len(wf)):
    firepoints=wf.iloc[[i]]
    firepoints=gpd.tools.sjoin(points, firepoints, predicate='within', how='inner')
    if len(firepoints)>0:
        firepoints=firepoints[(firepoints['acq_date']>=firepoints['FireDiscoveryDateTime_y']) & (firepoints['acq_date']<=firepoints['FireOutDateTime'])]
    pointsinfire=pd.concat([pointsinfire, firepoints], ignore_index=True)

pointsinfire=pointsinfire.sort_values(['IncidentName', 'acq_date', 'acq_time'])

fire = pointsinfire.groupby(['IncidentName', 'acq_date', 'acq_time'])
perimeter = gpd.GeoDataFrame()
for (name, date, time), group in fire:
    if len(group)>1:
        points = MultiPoint(group[['longitude', 'latitude']].to_numpy())
        polygon = points.convex_hull
        perimeter = perimeter.append({'IncidentName':name, 'acq_date':date, 'acq_time':time, 'geometry':polygon}, ignore_index=True)

perimeter = gpd.GeoDataFrame(perimeter)
perimeter['area'] = perimeter['geometry'].area

## stop here
pointsinfire['time']= round(pointsinfire['acq_time']/100).astype(int).astype(str)

pointsinfire

lat = 44.3
long = -118.5
start = '2019-08-20'
end = '2019-08-20'
url = 'https://archive-api.open-meteo.com/v1/archive'
params = {'latitude': lat, 'longitude': long, 'start_date': start, 'end_date': end, 'timezone': 'auto', 'temperature_unit': 'fahrenheit',
          'hourly': 'temperature_2m,dewpoint_2m,relativehumidity_2m,pressure_msl,precipitation,rain,snowfall,cloudcover,cloudcover_low,cloudcover_mid,cloudcover_high,windspeed_10m,windspeed_100m,winddirection_10m,winddirection_100m,windgusts_10m,soil_temperature_0_to_7cm,soil_temperature_7_to_28cm,soil_temperature_28_to_100cm,soil_temperature_100_to_255cm,soil_moisture_0_to_7cm,soil_moisture_7_to_28cm,soil_moisture_28_to_100cm,soil_moisture_100_to_255cm'}
res = requests.get(url, params = params)
#res = requests.get('https://archive-api.open-meteo.com/v1/archive?latitude=%7Blat%7D&longitude=%7Blong%7D&start_date=%7Bstart%7D&end_date=%7Bend%7D&hourly=relativehumidity_2m,pressure_msl,precipitation,rain,snowfall,cloudcover,cloudcover_low,cloudcover_mid,cloudcover_high,windspeed_10m,windspeed_100m,winddirection_10m,winddirection_100m,windgusts_10m,soil_temperature_7_to_28cm,soil_temperature_28_to_100cm,soil_temperature_100_to_255cm,soil_moisture_0_to_7cm,soil_moisture_7_to_28cm,soil_moisture_28_to_100cm,soil_moisture_100_to_255cm&timezone=auto&temperature_unit=fahrenheit%27)
response = json.loads(res.text)

response['hourly'].keys()

def get_weather_data(lat, long, start, end):
  url = 'https://archive-api.open-meteo.com/v1/archive'
  params = {'latitude': lat, 'longitude': long, 'start_date': start, 'end_date': end, 'temperature_unit': 'fahrenheit',
            'hourly': 'temperature_2m,dewpoint_2m,relativehumidity_2m,pressure_msl,precipitation,rain,snowfall,cloudcover,cloudcover_low,cloudcover_mid,cloudcover_high,windspeed_10m,windspeed_100m,winddirection_10m,winddirection_100m,windgusts_10m,soil_temperature_0_to_7cm,soil_temperature_7_to_28cm,soil_temperature_28_to_100cm,soil_temperature_100_to_255cm,soil_moisture_0_to_7cm,soil_moisture_7_to_28cm,soil_moisture_28_to_100cm,soil_moisture_100_to_255cm'}
  res = requests.get(url, params = params)
  #res = requests.get('https://archive-api.open-meteo.com/v1/archive?latitude=%7Blat%7D&longitude=%7Blong%7D&start_date=%7Bstart%7D&end_date=%7Bend%7D&hourly=relativehumidity_2m,pressure_msl,precipitation,rain,snowfall,cloudcover,cloudcover_low,cloudcover_mid,cloudcover_high,windspeed_10m,windspeed_100m,winddirection_10m,winddirection_100m,windgusts_10m,soil_temperature_7_to_28cm,soil_temperature_28_to_100cm,soil_temperature_100_to_255cm,soil_moisture_0_to_7cm,soil_moisture_7_to_28cm,soil_moisture_28_to_100cm,soil_moisture_100_to_255cm&timezone=auto&temperature_unit=fahrenheit%27)
  response = json.loads(res.text)
  status_code = res.status_code

  if status_code == 200:
    weather = response.pop('hourly')
    #response_df = pd.DataFrame.from_dict(response)
    weather_df = pd.DataFrame.from_dict(weather)

    return status_code, weather_df
  else:
    print(response)
    print(status_code)
    return status_code, None

#get_weather_data(lat, long, start, end)

fire3.info()

fire3['FireDiscoveryDateTime_y'] = pd.to_datetime(fire3['FireDiscoveryDateTime_y'])
fire3['discoveryTime'] = pd.to_datetime(fire3['FireDiscoveryDateTime_y']).dt.tz_localize(None)
fire3['containmentTime'] = pd.to_datetime(fire3['ContainmentDateTime_y']).dt.tz_localize(None)

#fire3['discoveryTime']
#fire3['containmentTime']

def date_string(date): #'2019-08-20'
  year = date.year
  month = date.month
  if month < 10:
    month = f'0{month}'
  day = date.day
  if day < 10:
    day = f'0{day}'

  dateString = f'{year}-{month}-{day}'
  return dateString

weather_res = []
for i in range(len(fire3)):
  lat = fire3.iloc[i]['coords'].y
  long = fire3.iloc[i]['coords'].x
  start = date_string(fire3.iloc[i]['discoveryTime'])
  end = date_string(fire3.iloc[i]['containmentTime'])
  status_code, res = get_weather_data(lat, long, start, end)
  if status_code == 200:
    res['IncidentName'] = fire3.iloc[i]['IncidentName']
    weather_res.append(res)
  else:
    print('got an error, idx', i)
    print(start, end)

"""#Weather Data Info"""

weather_data_df = pd.concat(weather_res)

weather_data_df1 = pd.concat(weather_res)

weather_data_df1.info()

weather_data_df.to_csv(BASE_PATH + '/WeatherDataDf.csv')

weather_data_df1.to_csv(BASE_PATH + '/WeatherDataDf1.csv')

weather_data_df.info()

weather_data_df.info()

weather_original = pd.read_csv(BASE_PATH + '/WeatherDataDf.csv')
weather1 = pd.read_csv(BASE_PATH + '/WeatherDataDf1.csv')

combined_weather = pd.concat([weather1,weather_original.iloc[:,1:26]],axis = 1)
#combined_weather.info()
final_weather_data = combined_weather.iloc[:,1:29]
final_weather_data.info()

"""#Final Weather Data"""

finalWeather = pd.read_csv(BASE_PATH + '/finalWeatherData.csv')
finalWeather.info()

weather_data_df = pd.read_csv(BASE_PATH + "/finalWeatherData.csv").iloc[:,1:27]
weather_data_df.corr()

pointsinfire.head()

pointsinfire['dateTime'] = pointsinfire['acq_date'] + pointsinfire['time']
for i in range(1,len(pointsinfire)):
  if pointsinfire.iloc[i]['IncidentName'] == pointsinfire.iloc[i-1]['IncidentName']:
    pointsinfire.iloc[i]['dateTime']

"""#Operations to Format DateTime"""

from datetime import datetime
pointsinfire['dateTime'] = pointsinfire['acq_date'].astype(str) + ' ' + pointsinfire['time']
#pointsinfire['dateTime'] = datetime.strptime(pointsinfire['dateTime'], '%Y-%d-%m %H')

time1 = datetime.strptime(pointsinfire.iloc[0]['dateTime'], '%Y-%m-%d %H')

pointsinfire['acq_date']

arr_of_isotimes = []
for i in range(len(pointsinfire)):
  #pointsinfire.iloc[i]['dateTime'] = datetime.strptime(pointsinfire.iloc[i]['dateTime'], '%Y-%m-%d %H')
  year = pointsinfire.iloc[i].acq_date.year
  day = pointsinfire.iloc[i].acq_date.day
  month =  pointsinfire.iloc[i].acq_date.month
  hour = int(pointsinfire.iloc[i].time)

  
  arr_of_isotimes.append(datetime(year, month, day, hour).isoformat())
  # temp = datetime(year, month, day, hour).isoformat()
  # print(year, month, day, hour)
  # print(temp)

pointsinfire['isotimes'] = arr_of_isotimes

pointsinfire['isotimes']

weather_data_df['time']

perimeter.head(4)

perimeter.acq_time = perimeter.acq_time//100

arr_of_isotimes = []
for i in range(len(perimeter)):
  #pointsinfire.iloc[i]['dateTime'] = datetime.strptime(pointsinfire.iloc[i]['dateTime'], '%Y-%m-%d %H')
  year = perimeter.iloc[i].acq_date.year
  day = perimeter.iloc[i].acq_date.day
  month =  perimeter.iloc[i].acq_date.month
  hour = perimeter.iloc[i].acq_time
  arr_of_isotimes.append(datetime(year, month, day, hour).isoformat())

perimeter['isotimes'] = arr_of_isotimes

perimeter

perimeter.info()

drop_arr = []
for i in range(len(perimeter)-1):
  if perimeter.iloc[i]['IncidentName']== perimeter.iloc[i+1]['IncidentName']:
    if perimeter.iloc[i]['area'] > perimeter.iloc[i+1]['area']:
      drop_arr.append(True)
    else:
      drop_arr.append(False)
  else:
    drop_arr.append(False)

drop_arr.append(False)

perimeter['drop'] = drop_arr

second_perimeter = perimeter[perimeter['drop']==False]

# don't run the for loop yet - returns NaN's
# loops for average of all variables in the weather data for the time duration
mwi = []
for i in range (1, len(second_perimeter)):
  if second_perimeter.iloc[i]['IncidentName'] == second_perimeter.iloc[i-1]['IncidentName']:
    start, end = second_perimeter.iloc[i-1]['isotimes'], second_perimeter.iloc[i]['isotimes'] 
    weatherIncident = weather_data_df[weather_data_df['IncidentName'] == second_perimeter.iloc[i-1]['IncidentName']]
    weatherIncident = weatherIncident[(weatherIncident['time'] >= start) & (weatherIncident['time'] <= end)]
    temp = weatherIncident.drop(['time'], axis=1)
    mean_weatherIncident = temp.mean()

    mean_weatherIncident = pd.DataFrame(mean_weatherIncident).T

    mean_weatherIncident['startTime'] = start
    mean_weatherIncident['endTime'] = end
    mean_weatherIncident['IncidentName'] = second_perimeter.iloc[i-1]['IncidentName']
    mwi.append(mean_weatherIncident)

second_mwi_df = pd.concat(mwi)

# don't run the for loop yet - returns NaN's
# loops for average of all variables in the weather data for the time duration
mwi = []
for i in range (1, len(perimeter)):
  if perimeter.iloc[i]['IncidentName'] == perimeter.iloc[i-1]['IncidentName']:
    start, end = perimeter.iloc[i-1]['isotimes'], perimeter.iloc[i]['isotimes'] 
    weatherIncident = weather_data_df[weather_data_df['IncidentName'] == perimeter.iloc[i-1]['IncidentName']]
    weatherIncident = weatherIncident[(weatherIncident['time'] >= start) & (weatherIncident['time'] <= end)]
    temp = weatherIncident.drop(['time'], axis=1)
    mean_weatherIncident = temp.mean()

    mean_weatherIncident = pd.DataFrame(mean_weatherIncident).T

    mean_weatherIncident['startTime'] = start
    mean_weatherIncident['endTime'] = end
    mean_weatherIncident['IncidentName'] = perimeter.iloc[i-1]['IncidentName']
    mwi.append(mean_weatherIncident)

mwi_df = pd.concat(mwi)

mwi_df.head()

mwi_df = mwi_df.sort_values(['IncidentName', 'startTime'], ascending=True)

second_mwi_df = second_mwi_df.sort_values(['IncidentName', 'startTime'], ascending=True)

mwi_df.head()

mwi_df.info()

perimeter.info()

perimeter1 = pd.merge(perimeter, mwi_df, left_on=['IncidentName', 'isotimes'], right_on=['IncidentName', 'endTime'], how='left')

second_perimeter1 = pd.merge(second_perimeter, second_mwi_df, left_on=['IncidentName', 'isotimes'], right_on=['IncidentName', 'endTime'], how='left')

perimeter1 = perimeter1.drop(['area_change', 'area_roc'], axis=1)

"""#Change in Area"""

area_change = [0]
area_roc = [0]

for i in range(1, len(second_perimeter1)):
  if second_perimeter1.iloc[i]['IncidentName']==second_perimeter1.iloc[i-1]['IncidentName']:
    area_change.append(second_perimeter1.iloc[i]['area']-second_perimeter1.iloc[i-1]['area'])
    area_roc.append((second_perimeter1.iloc[i]['area']-second_perimeter1.iloc[i-1]['area'])/second_perimeter1.iloc[i-1]['area'])
  else:
    area_change.append(0)
    area_roc.append(0)

area_change = [0]
area_roc = [0]

for i in range(1, len(perimeter1)):
  if perimeter1.iloc[i]['IncidentName']==perimeter1.iloc[i-1]['IncidentName']:
    area_change.append(perimeter1.iloc[i]['area']-perimeter1.iloc[i-1]['area'])
    area_roc.append((perimeter1.iloc[i]['area']-perimeter1.iloc[i-1]['area'])/perimeter1.iloc[i-1]['area'])
  else:
    area_change.append(0)
    area_roc.append(0)

second_perimeter1['area_change'] = area_change
second_perimeter1['area_roc'] = area_roc

perimeter1['area_change'] = area_change
perimeter1['area_roc'] = area_roc

second_perimeter1['area_roc'] = second_perimeter1['area_roc']*100

perimeter1['area_roc'] = perimeter1['area_roc']*100

perimeter1.head()

pointsinfire.head(4)

second_perimeter2 = pd.merge(second_perimeter1, pointsinfire, on=['IncidentName', 'isotimes'], how='left')

perimeter2 = pd.merge(perimeter1, pointsinfire, on=['IncidentName', 'isotimes'], how='left')

second_perimeter2.to_csv(BASE_PATH + '/fire_areas2.0.csv')

second_perimeter2

perimeter2.to_csv(BASE_PATH + '/fire_areas.csv')

date_string(fire3.iloc[-1]['discoveryTime'])

fire3.iloc[0]['discoveryTime'].year

response.keys()
#response['hourly'].keys()

fire3.info() #

gdf = pointsinfire.iloc[6:34, :2]
gdf = gpd.GeoDataFrame(gdf, geometry=gpd.points_from_xy(gdf.longitude, gdf.latitude))
radius=10
projected_crs= "EPSG:3857"
gdf = gdf.set_crs("EPSG:4326")
gdf_projected = gdf.to_crs(projected_crs)
center = gdf_projected.geometry.unary_union.centroid

perimeter_points = []
for bearing in range(0, 360, 1):
    perimeter = transform(Proj(init=projected_crs), Proj(init='EPSG:4326'), *transform(Proj(init='EPSG:4326'), Proj(init=projected_crs), center.x, center.y, 0.0), radius, bearing)
    perimeter_points.append((perimeter[1], perimeter[0]))
perimeter_polygon = Polygon(perimeter_points)
perimeter_polygon_projected = gpd.GeoSeries([perimeter_polygon], crs="EPSG:4326").to_crs(projected_crs).iloc[0]
points_within_perimeter = gdf_projected[gdf_projected.geometry.within(perimeter_polygon_projected)]
if len(points_within_perimeter) > 0:
    area_within_perimeter = points_within_perimeter.unary_union.convex_hull.area
    print(f'Area within perimeter: {area_within_perimeter:.2f} sq. meters')
else:
    print('No points within perimeter')

per = fireper.boundary.plot(linewidth=1, edcolor="black")
plt.show()

"""#CatBoost -> Implementation of Model and Validation"""

! pip install catboost

data = pd.read_csv(BASE_PATH + '/final_data_predictors1.csv')
data = data.iloc[:, 1:59]
fireWeathers = pd.read_csv(BASE_PATH + '/fire_areas2.0.csv').iloc[:,1:67]

import catboost as cb

from sklearn.model_selection import train_test_split

X = data.drop(['FireOutDateTime'], axis=1)
y = fireWeathers['area_roc']

temp = np.array(y)
valid = np.unique(temp)[:-2]
valid

invalid_idx = []
for i in range(len(y)):
  if y[i] not in valid:
    invalid_idx.append(i)

len(invalid_idx)

y = y.drop(index = invalid_idx)

X = X.drop(index=invalid_idx)

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)

import seaborn as sns
# import shap
# import load_boston
from matplotlib import pyplot as plt
# from sklearn.datasets 
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn.inspection import permutation_importance

train_dataset = cb.Pool(X_train, y_train) 
test_dataset = cb.Pool(X_val, y_val)

model = cb.CatBoostRegressor(loss_function='RMSE')

grid = {'iterations': [550],
        'learning_rate': [0.03],
        'depth': [15],
        'l2_leaf_reg': [40]}
model.grid_search(grid, train_dataset)

pred = model.predict(X_val)
rmse = (np.sqrt(mean_squared_error(y_val, pred)))
r2 = r2_score(y_val, pred)
print("Testing performance")
print('R2: {:.2f}'.format(r2))

"""#Importance of Different Features [Weather / Conditions]"""

sorted_feature_importance = model.feature_importances_.argsort()
cols = X.columns[sorted_feature_importance]
temp = model.feature_importances_[sorted_feature_importance]


plt.barh(cols[-10:], 
       temp[-10:], 
        color='turquoise')
plt.xlabel("CatBoost Feature Importance")

"""# XGBoost (Our Proposed Model for Larger Amount of Test Data)"""

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)

import seaborn as sns
# import shap
# import load_boston
from matplotlib import pyplot as plt
# from sklearn.datasets 
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn.inspection import permutation_importance
from sklearn.metrics import mean_squared_error

from xgboost.sklearn import XGBRegressor
import time

'''
******XGBoost*******
'''
clf = XGBRegressor(n_estimators=550,
                     max_depth=15,
                     max_leaves=40,
                     eta=0.04,
                     reg_lambda=1,
                     tree_method='hist',
                     eval_metric='rmse',
                     use_label_encoder=True,
                     random_state=1000,
                     n_jobs=-1)

clf.fit(X_train,y_train)

#check the runtime of the model
start = time.time() 
elapsed = time.time() - start
print(f'XGB Training ran in {elapsed:.5f} seconds')
y_pred = clf.predict(X_val)

mean_squared_error(y_val, y_pred)

mean_squared_error(y_val, y_pred)**0.5